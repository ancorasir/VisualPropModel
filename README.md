# Reconstructing Soft Robotic Touch via In-Finger Vision
Ning Guo<sup>#</sup>, Xudong Han<sup>#</sup>, Shuqiao Zhong, Zhiyuan Zhou, Jian Lin, Fang Wan**, Chaoyang Song*

<br>

## Graphic Abstract
![Fig0-GraphicalAbstract](https://github.com/ancorasir/VisualPropModel/assets/42087775/de914a2b-e024-4824-8990-49e253fe2e3c)

<br>

## Supplementary Videos

### Movie S1: Adaptive Capability of Soft Finger.
https://github.com/ancorasir/VisualPropModel/assets/42087775/05b125b4-1b54-4d8e-bcd3-d29f7ea373c0

### Movie S2: Interactive Shape Estimation.
https://github.com/ancorasir/VisualPropModel/assets/42087775/0a33e725-d765-4cde-85b9-1fc31300aa3b

### Movie S3: Dynamic Touch Path Estimation.
https://github.com/ancorasir/VisualPropModel/assets/42087775/91841472-132b-4e53-aab9-03cb88f4a2ec


### Movie S4: Human-Robot Collaboration with Soft Intelligence.
https://github.com/ancorasir/VisualPropModel/assets/42087775/2ab043ad-e5f0-45eb-8172-cafa87f1fed9


### Movie S5: Adaptive Grasping Enhanced by Touch Estimation.
https://github.com/ancorasir/VisualPropModel/assets/42087775/3fda6215-8862-4b5e-b8d7-574c12e1e14f

<br>

## Project Structure

The directory structure of this project looks like this:

```
├── checkpth                   <- Model checkpoint dir
│   
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Data configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── logs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Processing Raw Data,                        
│                             e.g. `RawDataProcessing.ipynb`.
│
├── src                    <- Source code
│   ├── data                     <- Data scripts
│   ├── models                   <- Model scripts
│   ├── utils                    <- Utility scripts
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── tests                  <- Tests of any kind
│
├── .project-root             <- File for inferring the position of project root directory
├── requirements.txt          <- File for installing python dependencies
└── README.md
```

<br>

##   Quickstart

```bash
# clone project
git clone https://github.com/ancorasir/VisualPropModel.git
cd VisualPropModel

```

<br>

<summary><b>DataSet</b></summary>
1、The recorded data for model training is located in the 'data/raw' folder \
2、The 'notebooks/RawDataProcessing.ipynb' file contains the necessary steps for processing raw data

<br>

<summary><b>Model Training</b></summary>

```bash
# train on CPU(default)
python src/train.py

# train on 1 GPU
python src/train.py trainer=gpu
```


<summary><b>Model Evaluation</b></summary>

```bash
python src/eval.py 
```

